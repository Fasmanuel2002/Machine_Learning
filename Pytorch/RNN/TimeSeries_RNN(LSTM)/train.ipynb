{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "49484e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c1123c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Period       Revenue  Sales_quantity  Average_cost  \\\n",
      "0  01.01.2015  1.601007e+07         12729.0   1257.763541   \n",
      "1  01.02.2015  1.580759e+07         11636.0   1358.507000   \n",
      "2  01.03.2015  2.204715e+07         15922.0   1384.697024   \n",
      "3  01.04.2015  1.881458e+07         15227.0   1235.606705   \n",
      "4  01.05.2015  1.402148e+07          8620.0   1626.621765   \n",
      "\n",
      "   The_average_annual_payroll_of_the_region  \n",
      "0                                30024676.0  \n",
      "1                                30024676.0  \n",
      "2                                30024676.0  \n",
      "3                                30024676.0  \n",
      "4                                30024676.0  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Month_value_1.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5378974b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of Nans: 0.26666666666666666\n"
     ]
    }
   ],
   "source": [
    "values = df.iloc[:, 1:5].values\n",
    "features = values.shape[1]\n",
    "total_df_size = df.size\n",
    "values_Nans = df.isna().sum().sum()\n",
    "print(f\"Total of Nans: {values_Nans / total_df_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dcc25bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/64/_d7wbs111535lmdv26qfwp000000gn/T/ipykernel_10438/646466139.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method=\"ffill\", inplace=True)\n",
      "/var/folders/64/_d7wbs111535lmdv26qfwp000000gn/T/ipykernel_10438/646466139.py:6: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method=\"bfill\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The total Nans of these DataSet is 26.6%\n",
    "We need to fill it out\n",
    "\"\"\"\n",
    "df.fillna(method=\"ffill\", inplace=True)\n",
    "df.fillna(method=\"bfill\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c61b07f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of Nans after fillna: 0.0\n"
     ]
    }
   ],
   "source": [
    "values = df.iloc[:, 1:5].values\n",
    "features = values.shape[1]\n",
    "total_df_size = df.size\n",
    "values_Nans = df.isna().sum().sum()\n",
    "print(f\"Total of Nans after fillna: {values_Nans / total_df_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d51c9742",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "values_evidence_normalize = scaler.fit_transform(values)\n",
    "\n",
    "X, y = [], []\n",
    "#Appending by the window_size that the \n",
    "window_size = 10\n",
    "for i in range(len(values_evidence_normalize) - window_size):\n",
    "    X.append(values_evidence_normalize[i: i + window_size])\n",
    "    y.append(values_evidence_normalize[i + window_size])\n",
    "    \n",
    "X = np.array(X).astype(np.float32)\n",
    "y = np.array(y).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0babbfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape: (60, 10, 4)\n",
      "Y train shape: (60, 4)\n",
      "X test shape: (26, 10, 4)\n",
      "Y test shape: (26, 4)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, y, test_size=0.30, shuffle=False\n",
    ")\n",
    "print(f\"X train shape: {X_train.shape}\")\n",
    "print(f\"Y train shape: {Y_train.shape}\")\n",
    "print(f\"X test shape: {X_test.shape}\")\n",
    "print(f\"Y test shape: {Y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d973c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(Y_train, dtype=torch.float32)\n",
    "x_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(Y_test, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bbad76fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, output=features):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        \n",
    "        self.LSTM_1 = nn.LSTM(input_size=X_train.shape[2],\n",
    "                              hidden_size=100, \n",
    "                              num_layers=1,\n",
    "                              batch_first=True)\n",
    "        self.LSTM_2 = nn.LSTM(input_size=100,\n",
    "                            hidden_size=75,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True)\n",
    "        self.LSTM_3 = nn.LSTM(input_size=75,\n",
    "                                hidden_size=50,\n",
    "                                num_layers=1,\n",
    "                                batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(50, out_features=output)\n",
    "    def forward(self, x):\n",
    "        for lstm in [self.LSTM_1, self.LSTM_2, self.LSTM_3]:\n",
    "            x, _ = lstm(x)\n",
    "        return self.fc(x[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2fe5182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = LSTM_Model(features)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.AdamW(params=rnn_model.parameters(),lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2a9c6509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 0.2700\n",
      "Epoch [2/200], Loss: 0.2686\n",
      "Epoch [3/200], Loss: 0.2672\n",
      "Epoch [4/200], Loss: 0.2658\n",
      "Epoch [5/200], Loss: 0.2643\n",
      "Epoch [6/200], Loss: 0.2629\n",
      "Epoch [7/200], Loss: 0.2615\n",
      "Epoch [8/200], Loss: 0.2601\n",
      "Epoch [9/200], Loss: 0.2588\n",
      "Epoch [10/200], Loss: 0.2574\n",
      "Epoch [11/200], Loss: 0.2560\n",
      "Epoch [12/200], Loss: 0.2546\n",
      "Epoch [13/200], Loss: 0.2531\n",
      "Epoch [14/200], Loss: 0.2517\n",
      "Epoch [15/200], Loss: 0.2503\n",
      "Epoch [16/200], Loss: 0.2489\n",
      "Epoch [17/200], Loss: 0.2475\n",
      "Epoch [18/200], Loss: 0.2461\n",
      "Epoch [19/200], Loss: 0.2446\n",
      "Epoch [20/200], Loss: 0.2432\n",
      "Epoch [21/200], Loss: 0.2417\n",
      "Epoch [22/200], Loss: 0.2403\n",
      "Epoch [23/200], Loss: 0.2388\n",
      "Epoch [24/200], Loss: 0.2373\n",
      "Epoch [25/200], Loss: 0.2358\n",
      "Epoch [26/200], Loss: 0.2343\n",
      "Epoch [27/200], Loss: 0.2327\n",
      "Epoch [28/200], Loss: 0.2312\n",
      "Epoch [29/200], Loss: 0.2296\n",
      "Epoch [30/200], Loss: 0.2280\n",
      "Epoch [31/200], Loss: 0.2264\n",
      "Epoch [32/200], Loss: 0.2248\n",
      "Epoch [33/200], Loss: 0.2231\n",
      "Epoch [34/200], Loss: 0.2214\n",
      "Epoch [35/200], Loss: 0.2197\n",
      "Epoch [36/200], Loss: 0.2179\n",
      "Epoch [37/200], Loss: 0.2162\n",
      "Epoch [38/200], Loss: 0.2143\n",
      "Epoch [39/200], Loss: 0.2125\n",
      "Epoch [40/200], Loss: 0.2106\n",
      "Epoch [41/200], Loss: 0.2087\n",
      "Epoch [42/200], Loss: 0.2067\n",
      "Epoch [43/200], Loss: 0.2047\n",
      "Epoch [44/200], Loss: 0.2026\n",
      "Epoch [45/200], Loss: 0.2005\n",
      "Epoch [46/200], Loss: 0.1983\n",
      "Epoch [47/200], Loss: 0.1961\n",
      "Epoch [48/200], Loss: 0.1939\n",
      "Epoch [49/200], Loss: 0.1915\n",
      "Epoch [50/200], Loss: 0.1891\n",
      "Epoch [51/200], Loss: 0.1867\n",
      "Epoch [52/200], Loss: 0.1841\n",
      "Epoch [53/200], Loss: 0.1815\n",
      "Epoch [54/200], Loss: 0.1789\n",
      "Epoch [55/200], Loss: 0.1761\n",
      "Epoch [56/200], Loss: 0.1733\n",
      "Epoch [57/200], Loss: 0.1704\n",
      "Epoch [58/200], Loss: 0.1674\n",
      "Epoch [59/200], Loss: 0.1643\n",
      "Epoch [60/200], Loss: 0.1611\n",
      "Epoch [61/200], Loss: 0.1578\n",
      "Epoch [62/200], Loss: 0.1544\n",
      "Epoch [63/200], Loss: 0.1510\n",
      "Epoch [64/200], Loss: 0.1474\n",
      "Epoch [65/200], Loss: 0.1437\n",
      "Epoch [66/200], Loss: 0.1400\n",
      "Epoch [67/200], Loss: 0.1361\n",
      "Epoch [68/200], Loss: 0.1322\n",
      "Epoch [69/200], Loss: 0.1281\n",
      "Epoch [70/200], Loss: 0.1240\n",
      "Epoch [71/200], Loss: 0.1198\n",
      "Epoch [72/200], Loss: 0.1156\n",
      "Epoch [73/200], Loss: 0.1113\n",
      "Epoch [74/200], Loss: 0.1069\n",
      "Epoch [75/200], Loss: 0.1026\n",
      "Epoch [76/200], Loss: 0.0982\n",
      "Epoch [77/200], Loss: 0.0939\n",
      "Epoch [78/200], Loss: 0.0897\n",
      "Epoch [79/200], Loss: 0.0856\n",
      "Epoch [80/200], Loss: 0.0816\n",
      "Epoch [81/200], Loss: 0.0779\n",
      "Epoch [82/200], Loss: 0.0744\n",
      "Epoch [83/200], Loss: 0.0711\n",
      "Epoch [84/200], Loss: 0.0683\n",
      "Epoch [85/200], Loss: 0.0658\n",
      "Epoch [86/200], Loss: 0.0637\n",
      "Epoch [87/200], Loss: 0.0622\n",
      "Epoch [88/200], Loss: 0.0610\n",
      "Epoch [89/200], Loss: 0.0603\n",
      "Epoch [90/200], Loss: 0.0600\n",
      "Epoch [91/200], Loss: 0.0601\n",
      "Epoch [92/200], Loss: 0.0603\n",
      "Epoch [93/200], Loss: 0.0607\n",
      "Epoch [94/200], Loss: 0.0611\n",
      "Epoch [95/200], Loss: 0.0615\n",
      "Epoch [96/200], Loss: 0.0616\n",
      "Epoch [97/200], Loss: 0.0616\n",
      "Epoch [98/200], Loss: 0.0614\n",
      "Epoch [99/200], Loss: 0.0611\n",
      "Epoch [100/200], Loss: 0.0607\n",
      "Epoch [101/200], Loss: 0.0601\n",
      "Epoch [102/200], Loss: 0.0596\n",
      "Epoch [103/200], Loss: 0.0591\n",
      "Epoch [104/200], Loss: 0.0586\n",
      "Epoch [105/200], Loss: 0.0583\n",
      "Epoch [106/200], Loss: 0.0580\n",
      "Epoch [107/200], Loss: 0.0578\n",
      "Epoch [108/200], Loss: 0.0576\n",
      "Epoch [109/200], Loss: 0.0575\n",
      "Epoch [110/200], Loss: 0.0575\n",
      "Epoch [111/200], Loss: 0.0575\n",
      "Epoch [112/200], Loss: 0.0575\n",
      "Epoch [113/200], Loss: 0.0575\n",
      "Epoch [114/200], Loss: 0.0575\n",
      "Epoch [115/200], Loss: 0.0575\n",
      "Epoch [116/200], Loss: 0.0575\n",
      "Epoch [117/200], Loss: 0.0574\n",
      "Epoch [118/200], Loss: 0.0574\n",
      "Epoch [119/200], Loss: 0.0573\n",
      "Epoch [120/200], Loss: 0.0572\n",
      "Epoch [121/200], Loss: 0.0571\n",
      "Epoch [122/200], Loss: 0.0570\n",
      "Epoch [123/200], Loss: 0.0568\n",
      "Epoch [124/200], Loss: 0.0567\n",
      "Epoch [125/200], Loss: 0.0566\n",
      "Epoch [126/200], Loss: 0.0565\n",
      "Epoch [127/200], Loss: 0.0564\n",
      "Epoch [128/200], Loss: 0.0563\n",
      "Epoch [129/200], Loss: 0.0562\n",
      "Epoch [130/200], Loss: 0.0561\n",
      "Epoch [131/200], Loss: 0.0560\n",
      "Epoch [132/200], Loss: 0.0559\n",
      "Epoch [133/200], Loss: 0.0559\n",
      "Epoch [134/200], Loss: 0.0558\n",
      "Epoch [135/200], Loss: 0.0557\n",
      "Epoch [136/200], Loss: 0.0556\n",
      "Epoch [137/200], Loss: 0.0555\n",
      "Epoch [138/200], Loss: 0.0554\n",
      "Epoch [139/200], Loss: 0.0554\n",
      "Epoch [140/200], Loss: 0.0553\n",
      "Epoch [141/200], Loss: 0.0552\n",
      "Epoch [142/200], Loss: 0.0551\n",
      "Epoch [143/200], Loss: 0.0550\n",
      "Epoch [144/200], Loss: 0.0549\n",
      "Epoch [145/200], Loss: 0.0548\n",
      "Epoch [146/200], Loss: 0.0547\n",
      "Epoch [147/200], Loss: 0.0546\n",
      "Epoch [148/200], Loss: 0.0545\n",
      "Epoch [149/200], Loss: 0.0544\n",
      "Epoch [150/200], Loss: 0.0543\n",
      "Epoch [151/200], Loss: 0.0542\n",
      "Epoch [152/200], Loss: 0.0541\n",
      "Epoch [153/200], Loss: 0.0540\n",
      "Epoch [154/200], Loss: 0.0539\n",
      "Epoch [155/200], Loss: 0.0538\n",
      "Epoch [156/200], Loss: 0.0537\n",
      "Epoch [157/200], Loss: 0.0536\n",
      "Epoch [158/200], Loss: 0.0535\n",
      "Epoch [159/200], Loss: 0.0534\n",
      "Epoch [160/200], Loss: 0.0532\n",
      "Epoch [161/200], Loss: 0.0531\n",
      "Epoch [162/200], Loss: 0.0530\n",
      "Epoch [163/200], Loss: 0.0529\n",
      "Epoch [164/200], Loss: 0.0528\n",
      "Epoch [165/200], Loss: 0.0527\n",
      "Epoch [166/200], Loss: 0.0526\n",
      "Epoch [167/200], Loss: 0.0525\n",
      "Epoch [168/200], Loss: 0.0523\n",
      "Epoch [169/200], Loss: 0.0522\n",
      "Epoch [170/200], Loss: 0.0521\n",
      "Epoch [171/200], Loss: 0.0520\n",
      "Epoch [172/200], Loss: 0.0519\n",
      "Epoch [173/200], Loss: 0.0517\n",
      "Epoch [174/200], Loss: 0.0516\n",
      "Epoch [175/200], Loss: 0.0515\n",
      "Epoch [176/200], Loss: 0.0514\n",
      "Epoch [177/200], Loss: 0.0513\n",
      "Epoch [178/200], Loss: 0.0511\n",
      "Epoch [179/200], Loss: 0.0510\n",
      "Epoch [180/200], Loss: 0.0509\n",
      "Epoch [181/200], Loss: 0.0507\n",
      "Epoch [182/200], Loss: 0.0506\n",
      "Epoch [183/200], Loss: 0.0505\n",
      "Epoch [184/200], Loss: 0.0504\n",
      "Epoch [185/200], Loss: 0.0502\n",
      "Epoch [186/200], Loss: 0.0501\n",
      "Epoch [187/200], Loss: 0.0500\n",
      "Epoch [188/200], Loss: 0.0498\n",
      "Epoch [189/200], Loss: 0.0497\n",
      "Epoch [190/200], Loss: 0.0496\n",
      "Epoch [191/200], Loss: 0.0494\n",
      "Epoch [192/200], Loss: 0.0493\n",
      "Epoch [193/200], Loss: 0.0492\n",
      "Epoch [194/200], Loss: 0.0490\n",
      "Epoch [195/200], Loss: 0.0489\n",
      "Epoch [196/200], Loss: 0.0488\n",
      "Epoch [197/200], Loss: 0.0486\n",
      "Epoch [198/200], Loss: 0.0485\n",
      "Epoch [199/200], Loss: 0.0484\n",
      "Epoch [200/200], Loss: 0.0482\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    rnn_model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = rnn_model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd945c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = rnn_model(x_test).numpy()\n",
    "\n",
    "mse = mean_squared_error(y_test.numpy(), y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test.numpy(), y_pred)\n",
    "\n",
    "print(\"\\n Results\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
